{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nim7KW49HbU5"
      },
      "source": [
        "![](http://static1.squarespace.com/static/56ccc8724c2f8548059fbcfe/58f6daea15d5dbcc64ef63aa/5cae2d770d92977242838baa/1557942174418/SW-DistractedDriving-Clean-1.jpg?format=1500w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUnV2kYxZyyo"
      },
      "source": [
        "# Milestone 1: Saliency maps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_ZLrG6VZkNc",
        "cellView": "form"
      },
      "source": [
        "#@title Re-import some libraries\n",
        "! pip3 install scipy==1.1.0 ### NOTE: YOU MAY NEED TO REINSTALL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sQUOJgW3Nu1"
      },
      "source": [
        "# RUN THE CODE BLOCK ABOVE, THEN RESTART THE RUNTIME.\n",
        "# THEN, THE FOLLOWING IMPORT STATEMENT WILL WORK.\n",
        "# DO THIS WITH YOUR INSTRUCTOR TO LEARN ABOUT INSTALLATION OF DEPENDENCIES!\n",
        "! pip install git+https://github.com/raghakot/keras-vis.git -U\n",
        "\n",
        "!pip install keras==2.2.4\n",
        "!pip install tensorflow-gpu==1.15.4\n",
        "\n",
        "from vis.visualization import visualize_saliency, visualize_cam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WtgHH-DbcRa"
      },
      "source": [
        "#@title Run this to download data and prepare our environment! { display-mode: \"form\" }\n",
        "\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def label_to_numpy(labels):\n",
        "  final_labels = np.zeros((len(labels), 4))\n",
        "  for i in range(len(labels)):\n",
        "    label = labels[i]\n",
        "    if label == 'Attentive':\n",
        "      final_labels[i,:] = np.array([1, 0, 0, 0])\n",
        "    if label == 'DrinkingCoffee':\n",
        "      final_labels[i,:] = np.array([0, 1, 0, 0])\n",
        "    if label == 'UsingMirror':\n",
        "      final_labels[i,:] = np.array([0, 0, 1, 0])\n",
        "    if label == 'UsingRadio':\n",
        "      final_labels[i,:] = np.array([0, 0, 0, 1])\n",
        "  return final_labels\n",
        "\n",
        "def augment(data, augmenter):\n",
        "  if len(data.shape) == 3:\n",
        "    return augmenter.augment_image(data)\n",
        "  if len(data.shape) == 4:\n",
        "    return augmenter.augment_images(data)\n",
        "\n",
        "def rotate(data, rotate):\n",
        "  fun = augmenters.Affine(rotate = rotate)\n",
        "  return augment(data, fun)\n",
        "\n",
        "def shear(data, shear):\n",
        "  fun = augmenters.Affine(shear = shear)\n",
        "  return augment(data, fun)\n",
        "\n",
        "def scale(data, scale):\n",
        "  fun = augmenters.Affine(scale = shear)\n",
        "  return augment(data, fun)\n",
        "\n",
        "def flip_left_right(data):\n",
        "  fun = augmenters.Fliplr()\n",
        "  return augment(data, fun)\n",
        "\n",
        "def flip_up_down(data):\n",
        "  fun = augmenters.Flipud()\n",
        "  return augment(data, fun)\n",
        "\n",
        "def remove_color(data, channel):\n",
        "  new_data = data.copy()\n",
        "  if len(data.shape) == 3:\n",
        "    new_data[:,:,channel] = 0\n",
        "    return new_data\n",
        "  if len(data.shape) == 4:\n",
        "    new_data[:,:,:,channel] = 0\n",
        "    return new_data\n",
        "\n",
        "class pkg:\n",
        "  #### DOWNLOADING AND LOADING DATA\n",
        "  def get_metadata(metadata_path, which_splits = ['train', 'test']):\n",
        "    '''returns metadata dataframe which contains columns of:\n",
        "       * index: index of data into numpy data\n",
        "       * class: class of image\n",
        "       * split: which dataset split is this a part of?\n",
        "    '''\n",
        "    metadata = pd.read_csv(metadata_path)\n",
        "    keep_idx = metadata['split'].isin(which_splits)\n",
        "    metadata = metadata[keep_idx]\n",
        "\n",
        "    # Get dataframes for each class.\n",
        "    df_coffee_train = metadata[(metadata['class'] == 'DrinkingCoffee') & \\\n",
        "                         (metadata['split'] == 'train')]\n",
        "    df_coffee_test = metadata[(metadata['class'] == 'DrinkingCoffee') & \\\n",
        "                         (metadata['split'] == 'test')]\n",
        "    df_mirror_train = metadata[(metadata['class'] == 'UsingMirror') & \\\n",
        "                         (metadata['split'] == 'train')]\n",
        "    df_mirror_test = metadata[(metadata['class'] == 'UsingMirror') & \\\n",
        "                         (metadata['split'] == 'test')]\n",
        "    df_attentive_train = metadata[(metadata['class'] == 'Attentive') & \\\n",
        "                         (metadata['split'] == 'train')]\n",
        "    df_attentive_test = metadata[(metadata['class'] == 'Attentive') & \\\n",
        "                         (metadata['split'] == 'test')]\n",
        "    df_radio_train = metadata[(metadata['class'] == 'UsingRadio') & \\\n",
        "                         (metadata['split'] == 'train')]\n",
        "    df_radio_test = metadata[(metadata['class'] == 'UsingRadio') & \\\n",
        "                         (metadata['split'] == 'test')]\n",
        "\n",
        "    # Get number of items in class with lowest number of images.\n",
        "    num_samples_train = min(df_coffee_train.shape[0], \\\n",
        "                            df_mirror_train.shape[0], \\\n",
        "                            df_attentive_train.shape[0], \\\n",
        "                            df_radio_train.shape[0])\n",
        "    num_samples_test = min(df_coffee_test.shape[0], \\\n",
        "                            df_mirror_test.shape[0], \\\n",
        "                            df_attentive_test.shape[0], \\\n",
        "                            df_radio_test.shape[0])\n",
        "\n",
        "    # Resample each of the classes and concatenate the images.\n",
        "    metadata_train = pd.concat([df_coffee_train.sample(num_samples_train), \\\n",
        "                          df_mirror_train.sample(num_samples_train), \\\n",
        "                          df_attentive_train.sample(num_samples_train), \\\n",
        "                          df_radio_train.sample(num_samples_train) ])\n",
        "    metadata_test = pd.concat([df_coffee_test.sample(num_samples_test), \\\n",
        "                          df_mirror_test.sample(num_samples_test), \\\n",
        "                          df_attentive_test.sample(num_samples_test), \\\n",
        "                          df_radio_test.sample(num_samples_test) ])\n",
        "\n",
        "    metadata = pd.concat( [metadata_train, metadata_test] )\n",
        "\n",
        "    return metadata\n",
        "\n",
        "  def get_data_split(split_name, flatten, all_data, metadata, image_shape):\n",
        "    '''\n",
        "    returns images (data), labels from folder of format [image_folder]/[split_name]/[class_name]/\n",
        "    flattens if flatten option is True\n",
        "    '''\n",
        "    # Get dataframes for each class.\n",
        "    df_coffee_train = metadata[(metadata['class'] == 'DrinkingCoffee') & \\\n",
        "                         (metadata['split'] == 'train')]\n",
        "    df_coffee_test = metadata[(metadata['class'] == 'DrinkingCoffee') & \\\n",
        "                         (metadata['split'] == 'test')]\n",
        "    df_mirror_train = metadata[(metadata['class'] == 'UsingMirror') & \\\n",
        "                         (metadata['split'] == 'train')]\n",
        "    df_mirror_test = metadata[(metadata['class'] == 'UsingMirror') & \\\n",
        "                         (metadata['split'] == 'test')]\n",
        "    df_attentive_train = metadata[(metadata['class'] == 'Attentive') & \\\n",
        "                         (metadata['split'] == 'train')]\n",
        "    df_attentive_test = metadata[(metadata['class'] == 'Attentive') & \\\n",
        "                         (metadata['split'] == 'test')]\n",
        "    df_radio_train = metadata[(metadata['class'] == 'UsingRadio') & \\\n",
        "                         (metadata['split'] == 'train')]\n",
        "    df_radio_test = metadata[(metadata['class'] == 'UsingRadio') & \\\n",
        "                         (metadata['split'] == 'test')]\n",
        "\n",
        "    # Get number of items in class with lowest number of images.\n",
        "    num_samples_train = min(df_coffee_train.shape[0], \\\n",
        "                            df_mirror_train.shape[0], \\\n",
        "                            df_attentive_train.shape[0], \\\n",
        "                            df_radio_train.shape[0])\n",
        "    num_samples_test = min(df_coffee_test.shape[0], \\\n",
        "                            df_mirror_test.shape[0], \\\n",
        "                            df_attentive_test.shape[0], \\\n",
        "                            df_radio_test.shape[0])\n",
        "\n",
        "    # Resample each of the classes and concatenate the images.\n",
        "    metadata_train = pd.concat([df_coffee_train.sample(num_samples_train), \\\n",
        "                          df_mirror_train.sample(num_samples_train), \\\n",
        "                          df_attentive_train.sample(num_samples_train), \\\n",
        "                          df_radio_train.sample(num_samples_train) ])\n",
        "    metadata_test = pd.concat([df_coffee_test.sample(num_samples_test), \\\n",
        "                          df_mirror_test.sample(num_samples_test), \\\n",
        "                          df_attentive_test.sample(num_samples_test), \\\n",
        "                          df_radio_test.sample(num_samples_test) ])\n",
        "\n",
        "    metadata = pd.concat( [metadata_train, metadata_test] )\n",
        "\n",
        "    sub_df = metadata[metadata['split'].isin([split_name])]\n",
        "    index  = sub_df['index'].values\n",
        "    labels = sub_df['class'].values\n",
        "    data = all_data[index,:]\n",
        "    if flatten:\n",
        "      data = data.reshape([-1, np.product(image_shape)])\n",
        "    return data, labels\n",
        "\n",
        "  def get_train_data(flatten, all_data, metadata, image_shape):\n",
        "    return get_data_split('train', flatten, all_data, metadata, image_shape)\n",
        "\n",
        "  def get_test_data(flatten, all_data, metadata, image_shape):\n",
        "    return get_data_split('test', flatten, all_data, metadata, image_shape)\n",
        "\n",
        "  def get_field_data(flatten, all_data, metadata, image_shape):\n",
        "    return get_data_split('field', flatten, all_data, metadata, image_shape)\n",
        "\n",
        "class helpers:\n",
        "  #### PLOTTING\n",
        "  def plot_image(data, num_ims, figsize=(8,6), labels = [], index = None, image_shape = [64,64,3]):\n",
        "    '''\n",
        "    if data is a single image, display that image\n",
        "\n",
        "    if data is a 4d stack of images, display that image\n",
        "    '''\n",
        "    print(data.shape)\n",
        "    num_dims   = len(data.shape)\n",
        "    num_labels = len(labels)\n",
        "\n",
        "    # reshape data if necessary\n",
        "    if num_dims == 1:\n",
        "      data = data.reshape(target_shape)\n",
        "    if num_dims == 2:\n",
        "      data = data.reshape(-1,image_shape[0],image_shape[1],image_shape[2])\n",
        "    num_dims   = len(data.shape)\n",
        "\n",
        "    # check if single or multiple images\n",
        "    if num_dims == 3:\n",
        "      if num_labels > 1:\n",
        "        print('Multiple labels does not make sense for single image.')\n",
        "        return\n",
        "\n",
        "      label = labels\n",
        "      if num_labels == 0:\n",
        "        label = ''\n",
        "      image = data\n",
        "\n",
        "    if num_dims == 4:\n",
        "      image = data[index, :]\n",
        "      label = labels[index]\n",
        "\n",
        "    # plot image of interest\n",
        "\n",
        "    nrows=int(np.sqrt(num_ims))\n",
        "    ncols=int(np.ceil(num_ims/nrows))\n",
        "    print(nrows,ncols)\n",
        "    count=0\n",
        "    if nrows==1 and ncols==1:\n",
        "      print('Label: %s'%label)\n",
        "      plt.imshow(image)\n",
        "      plt.show()\n",
        "    else:\n",
        "      print(labels)\n",
        "      fig = plt.figure(figsize=figsize)\n",
        "      for i in range(nrows):\n",
        "        for j in range(ncols):\n",
        "          if count<num_ims:\n",
        "            fig.add_subplot(nrows,ncols,count+1)\n",
        "            plt.imshow(image[count])\n",
        "            count+=1\n",
        "      fig.set_size_inches(18.5, 10.5)\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "\n",
        "  #### QUERYING AND COMBINING DATA\n",
        "  def get_misclassified_data(data, labels, predictions):\n",
        "    '''\n",
        "    Gets the data and labels that are misclassified in a classification task\n",
        "    Returns:\n",
        "    -missed_data\n",
        "    -missed_labels\n",
        "    -predicted_labels (corresponding to missed_labels)\n",
        "    -missed_index (indices of items in original dataset)\n",
        "    '''\n",
        "    missed_index     = np.where(np.abs(predictions.squeeze() - labels.squeeze()) > 0)[0]\n",
        "    missed_labels    = labels[missed_index]\n",
        "    missed_data      = data[missed_index,:]\n",
        "    predicted_labels = predictions[missed_index]\n",
        "    return missed_data, missed_labels, predicted_labels, missed_index\n",
        "\n",
        "  def combine_data(data_list, labels_list):\n",
        "    return np.concatenate(data_list, axis = 0), np.concatenate(labels_list, axis = 0)\n",
        "\n",
        "  def model_to_string(model):\n",
        "    import re\n",
        "    stringlist = []\n",
        "    model.summary(print_fn=lambda x: stringlist.append(x))\n",
        "    sms = \"\\n\".join(stringlist)\n",
        "    sms = re.sub('_\\d\\d\\d','', sms)\n",
        "    sms = re.sub('_\\d\\d','', sms)\n",
        "    sms = re.sub('_\\d','', sms)\n",
        "    return sms\n",
        "\n",
        "  def plot_acc(history, ax = None, xlabel = 'Epoch #'):\n",
        "    # i'm sorry for this function's code. i am so sorry.\n",
        "    history = history.history\n",
        "    history.update({'epoch':list(range(len(history['val_acc'])))})\n",
        "    history = pd.DataFrame.from_dict(history)\n",
        "\n",
        "    best_epoch = history.sort_values(by = 'val_acc', ascending = False).iloc[0]['epoch']\n",
        "\n",
        "    if not ax:\n",
        "      f, ax = plt.subplots(1,1)\n",
        "    sns.lineplot(x = 'epoch', y = 'val_acc', data = history, label = 'Validation', ax = ax)\n",
        "    sns.lineplot(x = 'epoch', y = 'acc', data = history, label = 'Training', ax = ax)\n",
        "    ax.axhline(0.25, linestyle = '--',color='red', label = 'Chance')\n",
        "    ax.axvline(x = best_epoch, linestyle = '--', color = 'green', label = 'Best Epoch')\n",
        "    ax.legend(loc = 1)\n",
        "    ax.set_ylim([0.01, 1])\n",
        "\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel('Accuracy (Fraction)')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "class models:\n",
        "  def DenseClassifier(hidden_layer_sizes, nn_params, dropout = 0.5):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape = nn_params['input_shape']))\n",
        "    for ilayer in hidden_layer_sizes:\n",
        "      model.add(Dense(ilayer, activation = 'relu'))\n",
        "      if dropout:\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Dense(units = nn_params['output_neurons'], activation = nn_params['output_activation']))\n",
        "    model.compile(loss=nn_params['loss'],\n",
        "                  optimizer=optimizers.SGD(lr=1e-4, momentum=0.95),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "  def CNNClassifier(num_hidden_layers, nn_params, dropout = 0.5):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3), input_shape=nn_params['input_shape'], padding = 'same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    for i in range(num_hidden_layers-1):\n",
        "        model.add(Conv2D(32, (3, 3), padding = 'same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(units = 128, activation = 'relu'))\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Dense(units = 64, activation = 'relu'))\n",
        "\n",
        "\n",
        "    model.add(Dense(units = nn_params['output_neurons'], activation = nn_params['output_activation']))\n",
        "\n",
        "    # initiate RMSprop optimizer\n",
        "    opt = tensorflow.keras.optimizers.RMSprop(lr=1e-4)\n",
        "\n",
        "    # Let's train the model using RMSprop\n",
        "    model.compile(loss=nn_params['loss'],\n",
        "                  optimizer=opt,\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "  def TransferClassifier(name, nn_params, trainable = True):\n",
        "    expert_dict = {'VGG16': VGG16,\n",
        "                   'VGG19': VGG19,\n",
        "                   'ResNet50':ResNet50,\n",
        "                   'DenseNet121':DenseNet121}\n",
        "\n",
        "    expert_conv = expert_dict[name](weights = 'imagenet',\n",
        "                                              include_top = False,\n",
        "                                              input_shape = nn_params['input_shape'])\n",
        "    for layer in expert_conv.layers:\n",
        "      layer.trainable = trainable\n",
        "\n",
        "    expert_model = Sequential()\n",
        "    expert_model.add(expert_conv)\n",
        "    expert_model.add(GlobalAveragePooling2D())\n",
        "\n",
        "    expert_model.add(Dense(128, activation = 'relu'))\n",
        "    expert_model.add(Dropout(0.3))\n",
        "\n",
        "    expert_model.add(Dense(64, activation = 'relu'))\n",
        "\n",
        "    expert_model.add(Dense(nn_params['output_neurons'], activation = nn_params['output_activation']))\n",
        "\n",
        "    expert_model.compile(loss = nn_params['loss'],\n",
        "                  optimizer = optimizers.SGD(lr=1e-4, momentum=0.95),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return expert_model\n",
        "\n",
        "import gdown\n",
        "import zipfile\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn import model_selection\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, MaxPooling2D, Dropout, Flatten, Reshape, Dense, Conv2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "import tensorflow.keras.optimizers as optimizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from tensorflow.keras.applications import VGG16, VGG19, ResNet50, DenseNet121\n",
        "\n",
        "from imgaug import augmenters\n",
        "\n",
        "### defining project variables\n",
        "# file variables\n",
        "image_data_url       = 'https://drive.google.com/uc?id=1qmTuUyn0525-612yS-wkp8gHB72Wv_XP'\n",
        "metadata_url         = 'https://drive.google.com/uc?id=1OfKnq3uIT29sXjWSZqOOpceig8Ul24OW'\n",
        "image_data_path      = './image_data.npy'\n",
        "metadata_path        = './metadata.csv'\n",
        "image_shape          = (64, 64, 3)\n",
        "\n",
        "# neural net parameters\n",
        "nn_params = {}\n",
        "nn_params['input_shape']       = image_shape\n",
        "nn_params['output_neurons']    = 4\n",
        "nn_params['loss']              = 'categorical_crossentropy'\n",
        "nn_params['output_activation'] = 'softmax'\n",
        "\n",
        "###\n",
        "gdown.download(image_data_url, image_data_path , True)\n",
        "gdown.download(metadata_url, metadata_path , True)\n",
        "\n",
        "\n",
        "### pre-loading all data of interest\n",
        "_all_data = np.load('image_data.npy')\n",
        "_metadata = pkg.get_metadata(metadata_path, ['train','test','field'])\n",
        "\n",
        "### preparing definitions\n",
        "# downloading and loading data\n",
        "get_data_split = pkg.get_data_split\n",
        "get_metadata    = lambda :                 pkg.get_metadata(metadata_path, ['train','test'])\n",
        "get_train_data  = lambda flatten = False : pkg.get_train_data(flatten = flatten, all_data = _all_data, metadata = _metadata, image_shape = image_shape)\n",
        "get_test_data   = lambda flatten = False : pkg.get_test_data(flatten = flatten, all_data = _all_data, metadata = _metadata, image_shape = image_shape)\n",
        "get_field_data  = lambda flatten = False : pkg.get_field_data(flatten = flatten, all_data = _all_data, metadata = _metadata, image_shape = image_shape)\n",
        "\n",
        "# plotting\n",
        "plot_image = lambda data, num_ims,figsize=(8,6), labels = [], index = None: helpers.plot_image(data = data, num_ims=num_ims, figsize=figsize,labels = labels, index = index, image_shape = image_shape);\n",
        "plot_acc       = lambda history: helpers.plot_acc(history)\n",
        "\n",
        "# querying and combining data\n",
        "model_to_string        = lambda model: helpers.model_to_string(model)\n",
        "get_misclassified_data = helpers.get_misclassified_data;\n",
        "combine_data           = helpers.combine_data;\n",
        "\n",
        "# models with input parameters\n",
        "DenseClassifier     = lambda hidden_layer_sizes: models.DenseClassifier(hidden_layer_sizes = hidden_layer_sizes, nn_params = nn_params);\n",
        "CNNClassifier       = lambda num_hidden_layers: models.CNNClassifier(num_hidden_layers, nn_params = nn_params);\n",
        "TransferClassifier  = lambda name: models.TransferClassifier(name = name, nn_params = nn_params);\n",
        "\n",
        "#monitor = ModelCheckpoint('./model.h5', monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', save_freq='epoch')\n",
        "monitor = ModelCheckpoint('./model.h5', monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21RARf1vaJvm"
      },
      "source": [
        "We can visualize which pixels in a given input image are the most important ones for the network. This means that we can show which pixels in the input image the network is looking at, when making a decision for a class. We can do this by calculating a gradient on each pixel with respect to a given class score.\n",
        "\n",
        "In the following example, we show a person drinking coffee, and we're interested in which pixels affect the networks \"coffee\" score the most! The resulting heatmap that indicates the magintude of the gradient for each pixel in the input image is called a **saliency map**.\n",
        "\n",
        "Before we get started, let's load in a CNN model as we did in the previous day. For convenience, we provide this code for you to get you started.\n",
        "\n",
        "**Run the below code. You are now very familiar with this code!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npou826JbeA8"
      },
      "source": [
        "train_data, train_labels = get_train_data(flatten=True)\n",
        "test_data, test_labels = get_test_data(flatten=True)\n",
        "\n",
        "train_data = train_data.reshape([-1, 64, 64, 3])\n",
        "test_data = test_data.reshape([-1, 64, 64, 3])\n",
        "\n",
        "# save string versions of labels\n",
        "train_labels_strings = train_labels\n",
        "test_labels_strings = test_labels\n",
        "\n",
        "# convert labels into numpy vectors (one-hot encoding!)\n",
        "train_labels = label_to_numpy(train_labels)\n",
        "test_labels = label_to_numpy(test_labels)\n",
        "\n",
        "dense = DenseClassifier(hidden_layer_sizes = (128,64))\n",
        "cnn = CNNClassifier(num_hidden_layers = 5)\n",
        "\n",
        "dense.fit(train_data, train_labels, epochs = 50, validation_data = (test_data, test_labels), shuffle = True, callbacks = [monitor])\n",
        "cnn.fit(train_data, train_labels, epochs = 50, validation_data = (test_data, test_labels), shuffle = True, callbacks = [monitor])\n",
        "\n",
        "print('Dense')\n",
        "plot_acc(dense.history)\n",
        "\n",
        "print('CNN')\n",
        "plot_acc(cnn.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQGPddVQuzGS"
      },
      "source": [
        "Now, let's view the saliency map of several of our images. We can visualize which pixels in a given input image are the most important ones for the network. This means that we can show which pixels in the input image the network is looking at, when making a decision for a class. We can do this by calculating a gradient on each pixel with respect to a given class score.\n",
        "\n",
        "In the following example, we show a person drinking coffee, and we're interested in which pixels affect the networks \"coffee\" score the most! The resulting heatmap that indicates the magnitude of the gradient for each pixel in the input image is called a **saliency map**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR3DCGLz6vXQ"
      },
      "source": [
        "Let's choose a random image to visualize. Feel free to select any image. Below, we select image 777 in our test data, but feel free to choose any image, either in training or testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwJ9RTSXrLqN"
      },
      "source": [
        "## Choose a random image to visualize.\n",
        "IMAGE_INDEX = 777\n",
        "img = test_data[IMAGE_INDEX]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-CgiUxev8VK"
      },
      "source": [
        "First, let's visualize the saliency of the first three layers of our CNN network. First, the input layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_4M_7IgvGyA"
      },
      "source": [
        "# Initialize Keras and Tensorflow sessions\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "session = keras.backend.get_session()\n",
        "init = tf.global_variables_initializer()\n",
        "session.run(init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZPmsBtP30hy"
      },
      "source": [
        "# Visualize the 1st layer (the second argument of the function).\n",
        "LAYER_TO_VISUALIZE = 1\n",
        "saliency_map = visualize_saliency(cnn, LAYER_TO_VISUALIZE, filter_indices = None, seed_input=img)\n",
        "plt.imshow(saliency_map)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teUyprYowKCr"
      },
      "source": [
        "## Exercise: Saliency Map\n",
        "\n",
        "The CNN we used has 18 convolutional layers. Use a for-loop to visualize the saliency map of each of these layers.\n",
        "\n",
        "*Hint: Above, we visualized the saliency map for the 1st layer. How would we do this for the ith layer?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0YnsKRYwM32"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy3iIKTswTRR"
      },
      "source": [
        "## Instructor-Led Discussion: Saliency maps\n",
        "\n",
        "With your group, discuss:\n",
        "\n",
        "**What differences do you notice between the saliency maps of the different layers of the CNN?**\n",
        "\n",
        "**What might be the underlying cause of these differences?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bm7RMvU5-in5"
      },
      "source": [
        "## (Optional) Exercise: Saliency of different classes\n",
        "\n",
        "Visualize the visual saliency at different layers of all 4 classes. Recall how we split the data array into 4 classes in Section 1 of this project. We do the same thing below -- feel free to choose an arbitrary image from each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BghlaWLssTWL"
      },
      "source": [
        "radio_train_data = train_data[train_labels_strings=='UsingRadio'] #grab all images whose corresponding label is 'UsingRadio'\n",
        "attentive_train_data = train_data[train_labels_strings=='Attentive'] #etc.\n",
        "coffee_train_data = train_data[train_labels_strings=='DrinkingCoffee']\n",
        "mirror_train_data = train_data[train_labels_strings=='UsingMirror']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VZKMiv2-2S8"
      },
      "source": [
        "# UsingRadio (fill in the first blank to grab any radio image)\n",
        "radio_img = __\n",
        "### YOUR CODE HERE\n",
        "\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7N8gnleowNaL"
      },
      "source": [
        "# Attentive (fill in the first blank to grab any attentive image)\n",
        "attentive_img = __\n",
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fSUEpeYwNyJ"
      },
      "source": [
        "# DrinkingCoffee (fill in the first blank to grab any coffee image)\n",
        "coffee_img = __\n",
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tzd1sf2uwODK"
      },
      "source": [
        "# UsingMirror (fill in the first blank to grab any mirror image)\n",
        "mirror_img = __\n",
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOFB2pezuGxV"
      },
      "source": [
        "# Milestone 2: Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwERVb37ylko"
      },
      "source": [
        "## Activity 2a. How did we do on predicting distracted drivers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXyQLXigDbtg"
      },
      "source": [
        "### Exercise (Coding) | Within a student group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yZBYHdmjstF"
      },
      "source": [
        "We use one of our best models from notebook2, the VGG16 classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_WZIH0FmXmH",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to import and train the VGG16 model\n",
        "\n",
        "#@title Instructor solution\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "\n",
        "# load the vgg network that is an 'expert' at 'imagenet' but do not include the FC layers\n",
        "vgg_expert = VGG16(weights = 'imagenet', include_top = False, input_shape = (64, 64, 3))\n",
        "\n",
        "# we add the first 12 layers of vgg to our own model vgg_model\n",
        "vgg_model = Sequential()\n",
        "vgg_model.add(vgg_expert)\n",
        "\n",
        "# and then add our own layers on top of it\n",
        "vgg_model.add(GlobalAveragePooling2D())\n",
        "vgg_model.add(Dense(1024, activation = 'relu'))\n",
        "vgg_model.add(Dropout(0.3))\n",
        "vgg_model.add(Dense(512, activation = 'relu'))\n",
        "vgg_model.add(Dropout(0.3))\n",
        "vgg_model.add(Dense(4, activation = 'softmax'))\n",
        "\n",
        "# finally, we build the vgg model and turn it on so we can use it!\n",
        "vgg_model.compile(loss = 'categorical_crossentropy',\n",
        "          optimizer = optimizers.SGD(lr=1e-4, momentum=0.95),\n",
        "          metrics=['accuracy'])\n",
        "\n",
        "# grab our augmented training manual\n",
        "# and hand it to our model to train\n",
        "vgg_model.fit(train_data, train_labels, epochs = 10, validation_data = (test_data, test_labels), shuffle = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBhDGS02VGHa"
      },
      "source": [
        "As we learned last week, total accuracy does not reflect all that we want to know about a model's performance. It's just one metric out of many possible metrics for evaluating models.\n",
        "\n",
        "In the case of detection of unsafe driving, we may be more interested in other quantities, such as 'how accurate were we on the distracted images?' or 'how accurate were we on the attentive category?' or 'how much of the attentive drivers were confused for distracted driving?' or vice versa.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_GjNUEBMSke"
      },
      "source": [
        "Our metrics for classification can be described in terms of a 'confusion matrix', shown below.\n",
        "\n",
        "![Confusion Matrix](https://cdn-images-1.medium.com/max/1600/1*Z54JgbS4DUwWSknhDCvNTQ.png)\n",
        "\n",
        "In a confusion matrix, we think in terms of 'actual' and 'predicted values'. If we take Distracted Driving (DrinkingCoffee, UsingMirror, or UsingRadio) = 1/Positive and Attentive = 0/Negative, then...\n",
        "\n",
        "* True positive: True distracted driving prediction: Distracted driving predicted as distracted driving\n",
        "* True negative: True attentive prediction: Attentive predicted as attentive\n",
        "* False positive: False distracted driving prediction: Attentive mistaken as distracted driving\n",
        "* False negative: False attentive prediction: Distracted driving mistaken as attentive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL0ns-b9GBym"
      },
      "source": [
        "The `sklearn` package makes calculating confusion matrices very quick! Its `metrics` submodule actually comes with a `confusion_matrix` tool. Let's start by grabbing that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhGqieeYu7rM"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvFawPv1NIyy"
      },
      "source": [
        "To use `confusion_matrix`, we need:\n",
        "* `labels`: the labels of the data (1 - DISTRACTED DRIVING or 0 - ATTENTIVE)\n",
        "* `predictions`: what our model thinks the labels are\n",
        "\n",
        "To get `predictions`, we have to give our model `test_labels`, our `test_data`, and ask it to give us `predictions`. We'll do that with\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztkgikl7zvdQ"
      },
      "source": [
        "predictions = vgg_model.predict_classes(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImWhImiQxnye"
      },
      "source": [
        "## Exercise: Confusion matrix for distracted vs. non-distracted driving\n",
        "\n",
        "First, write code below to calculate the accuracy of the classifier. Note that to complete this, you will need to transform the current 4-way classification into a binary (2-way) classification. That is, we will group all of UsingRadio, UsingMirror, and DrinkingCoffee into a single class representing all of distracted driving in general."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMv2nfVSBXcd"
      },
      "source": [
        "# First, covert the 4 labels to 2 numbers.\n",
        "# If the true label is 'Attentive', then we add 0 (for attentive) to final_labels. Otherwise,\n",
        "# for all other labels, we add a label of 1 (for distracted). Hint: .append() will add an element to a list!\n",
        "final_labels = []\n",
        "for label in test_labels_strings:\n",
        "  ### FILL IN THIS LOOP ###\n",
        "\n",
        "# Now, we change our final predictions to 0 and 1. That is, if the prediction\n",
        "# currently is 0, then add 0 (for attentive) to binary_predictions. Otherwise,\n",
        "# add 1 (for distracted).\n",
        "binary_predictions = []\n",
        "for label in predictions:\n",
        "  ### FILL IN THIS LOOP ###\n",
        "\n",
        "print('Accuracy is %d %%'%(accuracy_score(final_labels, binary_predictions)*100.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtPPu_s_CD_Z"
      },
      "source": [
        "### Discuss: Is this good accuracy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcFGwTMDkCAp"
      },
      "source": [
        "Now let's get our confusion matrix, and split it out into true positive, true negative, false positive, and false negative!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiQpzZLTvUOI"
      },
      "source": [
        "confusion = confusion_matrix(final_labels, binary_predictions)\n",
        "print(confusion)\n",
        "\n",
        "tp  = confusion[1][1]\n",
        "tn  = confusion[0][0]\n",
        "fp = confusion[0][1]\n",
        "fn = confusion[1][0]\n",
        "\n",
        "print('True positive: %d'%tp)\n",
        "print('True negative: %d'%tn)\n",
        "print('False positive: %d'%fp)\n",
        "print('False negative: %d'%fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKlghw6_krXL"
      },
      "source": [
        "We can visualize the confusion matrix with seaborn to make it easier for our eyes..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKt74I6mvhRh"
      },
      "source": [
        "# grab our plotting package\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.heatmap(confusion, annot = True, fmt = 'd', cbar_kws={'label':'count'});\n",
        "plt.ylabel('Actual');\n",
        "plt.xlabel('Predicted');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMTsIsU_v2KB"
      },
      "source": [
        "### Exercise (Discussion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8Tt6kTRkTvm"
      },
      "source": [
        "**Discuss with your instructor what you got and also...**\n",
        "\n",
        "What is more problematic? False positives or False negatives?\n",
        "\n",
        "Which of these metrics do we want to keep low?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy4Ka3RsCSTT"
      },
      "source": [
        "## Exercise: Precision and Recall Calculation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7ItfINHCfr0"
      },
      "source": [
        "Recall our discussion of precision and recall. Discuss with the course the difference between precision and recall. Then, print the precision and recall of the model and discuss with the class the implications of precision and recall in terms of distracted driving."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_0rWbNeCyGM"
      },
      "source": [
        "## YOUR CODE HERE\n",
        "\n",
        "## END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQIMMbfNDoeG"
      },
      "source": [
        "# Optional Exercise: Maintaining Privacy\n",
        "\n",
        "A common problem in video recording is maintaining privacy of users. If you have time, add privacy to all of our training and test data by modifying the images to have red boxes over the faces. Then, retrain the CNN and evaluate the performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSQ_h3lvEBsn"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8KmugJ_N9BQ"
      },
      "source": [
        "# Fin!\n"
      ]
    }
  ]
}